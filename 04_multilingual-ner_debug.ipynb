{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "734cf255-c0f2-4fc3-b2e5-23e710a59731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85fa0a15-7300-4faa-84bb-0aa7309ac051",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72ea34d-5416-4f62-ad12-d39cc9cdf164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6937f5-e1ea-4685-a8bb-7a6218d9ff3d",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c1e6d7b-7541-442a-a0d6-4b67532c168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import get_dataset_config_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72660247-2f90-4a7e-9bfe-10997f0497fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtreme_subsets = get_dataset_config_names(\"xtreme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c4f700-2713-4fc9-9338-98a6eb5da374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtreme_subsets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ac59b3f-f2b7-45f7-be7b-f6a8a1acb661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XNLI', 'tydiqa', 'SQuAD', 'PAN-X.af', 'PAN-X.ar']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtreme_subsets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e215d1-942d-47f4-a36b-045bd018f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9088fd4-ea48-459c-ac71-c0d7c22059d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(panx_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3df5f513-1ed3-4ea5-b4ff-da52c6a877c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg', 'PAN-X.bn', 'PAN-X.de']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_subsets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5133a52d-466b-4a7a-8109-97c3ae997a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "822daff2-bdca-4142-bcea-ee4660a49d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset xtreme (/home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b035f4bc48e4cb0a2d1b838407d539d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset(\"xtreme\", name=\"PAN-X.de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eeee23e-880d-4299-bf97-03162f5a694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17259a7c-0a8d-4698-bfb4-707e7ea1ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = [\"de\", \"fr\", \"it\", \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "361edb22-ed02-4f5c-8121-72e905aa3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fracs = [0.1, 0.1, 0.1, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8355158c-f329-4fc1-8b02-c38c3f88321f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DatasetDict in module datasets.dataset_dict:\n",
      "\n",
      "class DatasetDict(builtins.dict)\n",
      " |  A dictionary (dict of str: datasets.Dataset) with dataset transforms methods (map, filter, etc.)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DatasetDict\n",
      " |      builtins.dict\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, k) -> datasets.arrow_dataset.Dataset\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  align_labels_with_mapping(self, label2id: Dict, label_column: str) -> 'DatasetDict'\n",
      " |      Align the dataset's label ID and label name mapping to match an input :obj:`label2id` mapping.\n",
      " |      This is useful when you want to ensure that a model's predicted labels are aligned with the dataset.\n",
      " |      The alignment in done using the lowercase label names.\n",
      " |      \n",
      " |      Args:\n",
      " |          label2id (:obj:`dict`):\n",
      " |              The label name to ID mapping to align the dataset with.\n",
      " |          label_column (:obj:`str`):\n",
      " |              The column name of labels to align on.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> # dataset with mapping {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
      " |      >>> ds = load_dataset(\"glue\", \"mnli\", split=\"train\")\n",
      " |      >>> # mapping to align with\n",
      " |      >>> label2id = {'CONTRADICTION': 0, 'NEUTRAL': 1, 'ENTAILMENT': 2}\n",
      " |      >>> ds_aligned = ds.align_labels_with_mapping(label2id, \"label\")\n",
      " |      ```\n",
      " |  \n",
      " |  cast(self, features: datasets.features.features.Features) -> 'DatasetDict'\n",
      " |      Cast the dataset to a new set of features.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      You can also remove a column using :func:`Dataset.map` with `feature` but :func:`cast_`\n",
      " |      is in-place (doesn't copy the data to a new dataset) and is thus faster.\n",
      " |      \n",
      " |      Args:\n",
      " |          features (:class:`datasets.Features`): New features to cast the dataset to.\n",
      " |              The name and order of the fields in the features must match the current column names.\n",
      " |              The type of the data must also be convertible from one type to the other.\n",
      " |              For non-trivial conversion, e.g. string <-> ClassLabel you should use :func:`map` to update the Dataset.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> ds[\"train\"].features\n",
      " |      {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
      " |       'text': Value(dtype='string', id=None)}\n",
      " |      >>> new_features = ds[\"train\"].features.copy()\n",
      " |      >>> new_features['label'] = ClassLabel(names=['bad', 'good'])\n",
      " |      >>> new_features['text'] = Value('large_string')\n",
      " |      >>> ds = ds.cast(new_features)\n",
      " |      >>> ds[\"train\"].features\n",
      " |      {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
      " |       'text': Value(dtype='large_string', id=None)}\n",
      " |      ```\n",
      " |  \n",
      " |  cast_column(self, column: str, feature) -> 'DatasetDict'\n",
      " |      Cast column to feature for decoding.\n",
      " |      \n",
      " |      Args:\n",
      " |          column (:obj:`str`): Column name.\n",
      " |          feature (:class:`Feature`): Target feature.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`DatasetDict`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> ds[\"train\"].features\n",
      " |      {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
      " |       'text': Value(dtype='string', id=None)}\n",
      " |      >>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))\n",
      " |      >>> ds[\"train\"].features\n",
      " |      {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
      " |       'text': Value(dtype='string', id=None)}\n",
      " |      ```\n",
      " |  \n",
      " |  class_encode_column(self, column: str, include_nulls: bool = False) -> 'DatasetDict'\n",
      " |      Casts the given column as :obj:``datasets.features.ClassLabel`` and updates the tables.\n",
      " |      \n",
      " |      Args:\n",
      " |          column (`str`): The name of the column to cast\n",
      " |          include_nulls (`bool`, default `False`):\n",
      " |              Whether to include null values in the class labels. If True, the null values will be encoded as the `\"None\"` class label.\n",
      " |      \n",
      " |              *New in version 1.14.2*\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"boolq\")\n",
      " |      >>> ds[\"train\"].features\n",
      " |      {'answer': Value(dtype='bool', id=None),\n",
      " |       'passage': Value(dtype='string', id=None),\n",
      " |       'question': Value(dtype='string', id=None)}\n",
      " |      >>> ds = ds.class_encode_column(\"answer\")\n",
      " |      >>> ds[\"train\"].features\n",
      " |      {'answer': ClassLabel(num_classes=2, names=['False', 'True'], id=None),\n",
      " |       'passage': Value(dtype='string', id=None),\n",
      " |       'question': Value(dtype='string', id=None)}\n",
      " |      ```\n",
      " |  \n",
      " |  cleanup_cache_files(self) -> Dict[str, int]\n",
      " |      Clean up all cache files in the dataset cache directory, excepted the currently used cache file if there is one.\n",
      " |      Be careful when running this command that no other process is currently using other cache files.\n",
      " |      \n",
      " |      Return:\n",
      " |          Dict with the number of removed files for each split\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> ds.cleanup_cache_files()\n",
      " |      {'test': 0, 'train': 0, 'validation': 0}\n",
      " |      ```\n",
      " |  \n",
      " |  filter(self, function, with_indices=False, input_columns: Union[str, List[str], NoneType] = None, batched: bool = False, batch_size: Optional[int] = 1000, keep_in_memory: bool = False, load_from_cache_file: bool = True, cache_file_names: Optional[Dict[str, Optional[str]]] = None, writer_batch_size: Optional[int] = 1000, fn_kwargs: Optional[dict] = None, num_proc: Optional[int] = None, desc: Optional[str] = None) -> 'DatasetDict'\n",
      " |      Apply a filter function to all the elements in the table in batches\n",
      " |      and update the table so that the dataset only includes examples according to the filter function.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Args:\n",
      " |          function (`callable`): with one of the following signature:\n",
      " |              - ``function(example: Dict[str, Any]) -> bool`` if ``with_indices=False, batched=False``\n",
      " |              - ``function(example: Dict[str, Any], indices: int) -> bool`` if ``with_indices=True, batched=False``\n",
      " |              - ``function(example: Dict[str, List]) -> List[bool]`` if ``with_indices=False, batched=True``\n",
      " |              - ``function(example: Dict[str, List], indices: List[int]) -> List[bool]`` if ``with_indices=True, batched=True``\n",
      " |          with_indices (`bool`, defaults to `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n",
      " |          input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): The columns to be passed into `function` as\n",
      " |              positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n",
      " |          batched (`bool`, defaults to `False`): Provide batch of examples to `function`\n",
      " |          batch_size (:obj:`int`, optional, defaults to `1000`): Number of examples per batch provided to `function` if `batched=True`\n",
      " |              `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n",
      " |          keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          cache_file_names (`Optional[Dict[str, str]]`, defaults to `None`): Provide the name of a path for the cache file. It is used to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |              You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
      " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n",
      " |          fn_kwargs (:obj:`Dict`, optional, defaults to `None`): Keyword arguments to be passed to `function`\n",
      " |          num_proc (:obj:`int`, optional, defaults to `None`): Number of processes for multiprocessing. By default it doesn't\n",
      " |              use multiprocessing.\n",
      " |          desc (:obj:`str`, optional, defaults to `None`): Meaningful description to be displayed alongside with the progress bar while filtering examples.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> ds.filter(lambda x: x[\"label\"] == 1)\n",
      " |      DatasetDict({\n",
      " |          train: Dataset({\n",
      " |              features: ['text', 'label'],\n",
      " |              num_rows: 4265\n",
      " |          })\n",
      " |          validation: Dataset({\n",
      " |              features: ['text', 'label'],\n",
      " |              num_rows: 533\n",
      " |          })\n",
      " |          test: Dataset({\n",
      " |              features: ['text', 'label'],\n",
      " |              num_rows: 533\n",
      " |          })\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  flatten(self, max_depth=16) -> 'DatasetDict'\n",
      " |      Flatten the Apache Arrow Table of each split (nested features are flatten).\n",
      " |      Each column with a struct type is flattened into one column per struct field.\n",
      " |      Other columns are left unchanged.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"squad\")\n",
      " |      >>> ds[\"train\"].features\n",
      " |      {'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n",
      " |       'context': Value(dtype='string', id=None),\n",
      " |       'id': Value(dtype='string', id=None),\n",
      " |       'question': Value(dtype='string', id=None),\n",
      " |       'title': Value(dtype='string', id=None)}\n",
      " |      >>> ds.flatten()\n",
      " |      DatasetDict({\n",
      " |          train: Dataset({\n",
      " |              features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
      " |              num_rows: 87599\n",
      " |          })\n",
      " |          validation: Dataset({\n",
      " |              features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
      " |              num_rows: 10570\n",
      " |          })\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  formatted_as(self, type: Optional[str] = None, columns: Optional[List] = None, output_all_columns: bool = False, **format_kwargs)\n",
      " |      To be used in a `with` statement. Set ``__getitem__`` return format (type and columns)\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Args:\n",
      " |          type (:obj:`str`, optional): output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow']\n",
      " |              None means ``__getitem__`` returns python objects (default)\n",
      " |          columns (:obj:`List[str]`, optional): columns to format in the output\n",
      " |              None means ``__getitem__`` returns all columns (default)\n",
      " |          output_all_columns (:obj:`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n",
      " |          **format_kwargs (additional keyword arguments): keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |  \n",
      " |  map(self, function: Optional[Callable] = None, with_indices: bool = False, with_rank: bool = False, input_columns: Union[str, List[str], NoneType] = None, batched: bool = False, batch_size: Optional[int] = 1000, drop_last_batch: bool = False, remove_columns: Union[str, List[str], NoneType] = None, keep_in_memory: bool = False, load_from_cache_file: bool = True, cache_file_names: Optional[Dict[str, Optional[str]]] = None, writer_batch_size: Optional[int] = 1000, features: Optional[datasets.features.features.Features] = None, disable_nullable: bool = False, fn_kwargs: Optional[dict] = None, num_proc: Optional[int] = None, desc: Optional[str] = None) -> 'DatasetDict'\n",
      " |      Apply a function to all the elements in the table (individually or in batches)\n",
      " |      and update the table (if function does updated examples).\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Args:\n",
      " |          function (`callable`): with one of the following signature:\n",
      " |              - `function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and `with_indices=False`\n",
      " |              - `function(example: Dict[str, Any], indices: int) -> Dict[str, Any]` if `batched=False` and `with_indices=True`\n",
      " |              - `function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and `with_indices=False`\n",
      " |              - `function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List]` if `batched=True` and `with_indices=True`\n",
      " |      \n",
      " |              For advanced usage, the function can also return a `pyarrow.Table`.\n",
      " |              Moreover if your function returns nothing (`None`), then `map` will run your function and return the dataset unchanged.\n",
      " |      \n",
      " |          with_indices (`bool`, defaults to `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n",
      " |          with_rank (:obj:`bool`, default `False`): Provide process rank to `function`. Note that in this case the\n",
      " |              signature of `function` should be `def function(example[, idx], rank): ...`.\n",
      " |          input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): The columns to be passed into `function` as\n",
      " |              positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n",
      " |          batched (`bool`, defaults to `False`): Provide batch of examples to `function`\n",
      " |          batch_size (:obj:`int`, optional, defaults to `1000`): Number of examples per batch provided to `function` if `batched=True`\n",
      " |              `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n",
      " |          drop_last_batch (:obj:`bool`, default `False`): Whether a last batch smaller than the batch_size should be\n",
      " |              dropped instead of being processed by the function.\n",
      " |          remove_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): Remove a selection of columns while doing the mapping.\n",
      " |              Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n",
      " |              columns with names in `remove_columns`, these columns will be kept.\n",
      " |          keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          cache_file_names (`Optional[Dict[str, str]]`, defaults to `None`): Provide the name of a path for the cache file. It is used to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |              You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
      " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n",
      " |          features (`Optional[datasets.Features]`, defaults to `None`): Use a specific Features to store the cache file\n",
      " |              instead of the automatically generated one.\n",
      " |          disable_nullable (`bool`, defaults to `False`): Disallow null values in the table.\n",
      " |          fn_kwargs (:obj:`Dict`, optional, defaults to `None`): Keyword arguments to be passed to `function`\n",
      " |          num_proc (:obj:`int`, optional, defaults to `None`): Number of processes for multiprocessing. By default it doesn't\n",
      " |              use multiprocessing.\n",
      " |          desc (:obj:`str`, optional, defaults to `None`): Meaningful description to be displayed alongside with the progress bar while mapping examples.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> def add_prefix(example):\n",
      " |      ...     example[\"text\"] = \"Review: \" + example[\"text\"]\n",
      " |      ...     return example\n",
      " |      >>> ds = ds.map(add_prefix)\n",
      " |      >>> ds[\"train\"][0:3][\"text\"]\n",
      " |      ['Review: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
      " |       'Review: the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .',\n",
      " |       'Review: effective but too-tepid biopic']\n",
      " |      \n",
      " |      # process a batch of examples\n",
      " |      >>> ds = ds.map(lambda example: tokenizer(example[\"text\"]), batched=True)\n",
      " |      # set number of processors\n",
      " |      >>> ds = ds.map(add_prefix, num_proc=4)\n",
      " |      ```\n",
      " |  \n",
      " |  prepare_for_task(self, task: Union[str, datasets.tasks.base.TaskTemplate], id: int = 0) -> 'DatasetDict'\n",
      " |      Prepare a dataset for the given task by casting the dataset's [`Features`] to standardized column names and types as detailed in [datasets.tasks](/docs/datasets/package_reference/task_templates).\n",
      " |      \n",
      " |      Casts [`datasets.DatasetInfo.features`] according to a task-specific schema. Intended for single-use only, so all task templates are removed from [`datasets.DatasetInfo.task_templates`] after casting.\n",
      " |      \n",
      " |      Args:\n",
      " |          task (`Union[str, TaskTemplate]`): The task to prepare the dataset for during training and evaluation. If `str`, supported tasks include:\n",
      " |      \n",
      " |              - `\"text-classification\"`\n",
      " |              - `\"question-answering\"`\n",
      " |      \n",
      " |              If [`TaskTemplate`], must be one of the task templates in [`datasets.tasks`](/docs/datasets/package_reference/task_templates).\n",
      " |          id (`int`, defaults to 0): The id required to unambiguously identify the task template when multiple task templates of the same type are supported.\n",
      " |  \n",
      " |  push_to_hub(self, repo_id, private: Optional[bool] = False, token: Optional[str] = None, branch: NoneType = None, max_shard_size: Union[int, str] = '500MB', shard_size: Optional[int] = 'deprecated', embed_external_files: bool = True)\n",
      " |      Pushes the ``DatasetDict`` to the hub as a Parquet dataset.\n",
      " |      The ``DatasetDict`` is pushed using HTTP requests and does not need to have neither git or git-lfs installed.\n",
      " |      \n",
      " |      Each dataset split will be pushed independently. The pushed dataset will keep the original split names.\n",
      " |      \n",
      " |      The resulting Parquet files are self-contained by default: if your dataset contains :class:`Image` or :class:`Audio`\n",
      " |      data, the Parquet files will store the bytes of your images or audio files.\n",
      " |      You can disable this by setting `embed_external_files` to False.\n",
      " |      \n",
      " |      Args:\n",
      " |          repo_id (:obj:`str`):\n",
      " |              The ID of the repository to push to in the following format: ``<user>/<dataset_name>`` or\n",
      " |              ``<org>/<dataset_name>``. Also accepts ``<dataset_name>``, which will default to the namespace\n",
      " |              of the logged-in user.\n",
      " |          private (Optional :obj:`bool`):\n",
      " |              Whether the dataset repository should be set to private or not. Only affects repository creation:\n",
      " |              a repository that already exists will not be affected by that parameter.\n",
      " |          token (Optional :obj:`str`):\n",
      " |              An optional authentication token for the Hugging Face Hub. If no token is passed, will default\n",
      " |              to the token saved locally when logging in with ``huggingface-cli login``. Will raise an error\n",
      " |              if no token is passed and the user is not logged-in.\n",
      " |          branch (Optional :obj:`str`):\n",
      " |              The git branch on which to push the dataset.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"500MB\"`):\n",
      " |              The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit\n",
      " |              (like `\"500MB\"` or `\"1GB\"`).\n",
      " |          shard_size (Optional :obj:`int`):\n",
      " |              Deprecated: 'shard_size' was renamed to 'max_shard_size' in version 2.1.1 and will be removed in 2.4.0.\n",
      " |          embed_external_files (:obj:`bool`, default ``True``):\n",
      " |              Whether to embed file bytes in the shards.\n",
      " |              In particular, this will do the following before the push for the fields of type:\n",
      " |      \n",
      " |              - :class:`Audio` and class:`Image`: remove local path information and embed file content in the Parquet files.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\")\n",
      " |      ```\n",
      " |  \n",
      " |  remove_columns(self, column_names: Union[str, List[str]]) -> 'DatasetDict'\n",
      " |      Remove one or several column(s) from each split in the dataset\n",
      " |      and the features associated to the column(s).\n",
      " |      \n",
      " |      The transformation is applied to all the splits of the dataset dictionary.\n",
      " |      \n",
      " |      You can also remove a column using :func:`Dataset.map` with `remove_columns` but the present method\n",
      " |      is in-place (doesn't copy the data to a new dataset) and is thus faster.\n",
      " |      \n",
      " |      Args:\n",
      " |          column_names (:obj:`Union[str, List[str]]`): Name of the column(s) to remove.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> ds.remove_columns(\"label\")\n",
      " |      DatasetDict({\n",
      " |          train: Dataset({\n",
      " |              features: ['text'],\n",
      " |              num_rows: 8530\n",
      " |          })\n",
      " |          validation: Dataset({\n",
      " |              features: ['text'],\n",
      " |              num_rows: 1066\n",
      " |          })\n",
      " |          test: Dataset({\n",
      " |              features: ['text'],\n",
      " |              num_rows: 1066\n",
      " |          })\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  rename_column(self, original_column_name: str, new_column_name: str) -> 'DatasetDict'\n",
      " |      Rename a column in the dataset and move the features associated to the original column under the new column name.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      You can also rename a column using :func:`Dataset.map` with `remove_columns` but the present method:\n",
      " |          - takes care of moving the original features under the new column name.\n",
      " |          - doesn't copy the data to a new dataset and is thus much faster.\n",
      " |      \n",
      " |      Args:\n",
      " |          original_column_name (:obj:`str`): Name of the column to rename.\n",
      " |          new_column_name (:obj:`str`): New name for the column.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> ds.rename_column(\"label\", \"label_new\")\n",
      " |      DatasetDict({\n",
      " |          train: Dataset({\n",
      " |              features: ['text', 'label_new'],\n",
      " |              num_rows: 8530\n",
      " |          })\n",
      " |          validation: Dataset({\n",
      " |              features: ['text', 'label_new'],\n",
      " |              num_rows: 1066\n",
      " |          })\n",
      " |          test: Dataset({\n",
      " |              features: ['text', 'label_new'],\n",
      " |              num_rows: 1066\n",
      " |          })\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  rename_columns(self, column_mapping: Dict[str, str]) -> 'DatasetDict'\n",
      " |      Rename several columns in the dataset, and move the features associated to the original columns under\n",
      " |      the new column names.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Args:\n",
      " |          column_mapping (:obj:`Dict[str, str]`): A mapping of columns to rename to their new names\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`DatasetDict`: A copy of the dataset with renamed columns\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> ds.rename_columns({'text': 'text_new', 'label': 'label_new'})\n",
      " |      DatasetDict({\n",
      " |          train: Dataset({\n",
      " |              features: ['text_new', 'label_new'],\n",
      " |              num_rows: 8530\n",
      " |          })\n",
      " |          validation: Dataset({\n",
      " |              features: ['text_new', 'label_new'],\n",
      " |              num_rows: 1066\n",
      " |          })\n",
      " |          test: Dataset({\n",
      " |              features: ['text_new', 'label_new'],\n",
      " |              num_rows: 1066\n",
      " |          })\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  reset_format(self)\n",
      " |      Reset ``__getitem__`` return format to python objects and all columns.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Same as ``self.set_format()``\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      >>> ds = ds.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=True), batched=True)\n",
      " |      >>> ds.set_format(type=\"numpy\", columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      " |      >>> ds[\"train\"].format\n",
      " |      {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': 'numpy'}\n",
      " |      >>> ds.reset_format()\n",
      " |      >>> ds[\"train\"].format\n",
      " |      {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': None}\n",
      " |      ```\n",
      " |  \n",
      " |  save_to_disk(self, dataset_dict_path: str, fs=None)\n",
      " |      Saves a dataset dict to a filesystem using either :class:`~filesystems.S3FileSystem` or\n",
      " |      ``fsspec.spec.AbstractFileSystem``.\n",
      " |      \n",
      " |      For :class:`Image` and :class:`Audio` data:\n",
      " |      \n",
      " |      If your images and audio files are local files, then the resulting arrow file will store paths to these files.\n",
      " |      If you want to include the bytes or your images or audio files instead, you must `read()` those files first.\n",
      " |      This can be done by storing the \"bytes\" instead of the \"path\" of the images or audio files:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> def read_image_file(example):\n",
      " |      ...     with open(example[\"image\"].filename, \"rb\") as f:\n",
      " |      ...         return {\"image\": {\"bytes\": f.read()}}\n",
      " |      >>> ds = ds.map(read_image_file)\n",
      " |      >>> ds.save_to_disk(\"path/to/dataset/dir\")\n",
      " |      ```\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> def read_audio_file(example):\n",
      " |      ...     with open(example[\"audio\"][\"path\"], \"rb\") as f:\n",
      " |      ...         return {\"audio\": {\"bytes\": f.read()}}\n",
      " |      >>> ds = ds.map(read_audio_file)\n",
      " |      >>> ds.save_to_disk(\"path/to/dataset/dir\")\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset_dict_path (``str``): Path (e.g. `dataset/train`) or remote URI\n",
      " |              (e.g. `s3://my-bucket/dataset/train`) of the dataset dict directory where the dataset dict will be\n",
      " |              saved to.\n",
      " |          fs (:class:`~filesystems.S3FileSystem`, ``fsspec.spec.AbstractFileSystem``, optional, defaults ``None``):\n",
      " |              Instance of the remote filesystem used to download the files from.\n",
      " |  \n",
      " |  set_format(self, type: Optional[str] = None, columns: Optional[List] = None, output_all_columns: bool = False, **format_kwargs)\n",
      " |      Set ``__getitem__`` return format (type and columns)\n",
      " |      The format is set for every dataset in the dataset dictionary\n",
      " |      \n",
      " |      Args:\n",
      " |          type (:obj:`str`, optional): output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow']\n",
      " |              None means ``__getitem__`` returns python objects (default)\n",
      " |          columns (:obj:`List[str]`, optional): columns to format in the output.\n",
      " |              None means ``__getitem__`` returns all columns (default).\n",
      " |          output_all_columns (:obj:`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n",
      " |          **format_kwargs (additional keyword arguments): keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |      \n",
      " |      It is possible to call ``map`` after calling ``set_format``. Since ``map`` may add new columns, then the list of formatted columns\n",
      " |      gets updated. In this case, if you apply ``map`` on a dataset to add a new column, then this column will be formatted:\n",
      " |      \n",
      " |          new formatted columns = (all columns - previously unformatted columns)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      >>> ds = ds.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=True), batched=True)\n",
      " |      >>> ds.set_format(type=\"numpy\", columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      " |      >>> ds[\"train\"].format\n",
      " |      {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': 'numpy'}\n",
      " |      ```\n",
      " |  \n",
      " |  set_transform(self, transform: Optional[Callable], columns: Optional[List] = None, output_all_columns: bool = False)\n",
      " |      Set ``__getitem__`` return format using this transform. The transform is applied on-the-fly on batches when ``__getitem__`` is called.\n",
      " |      The transform is set for every dataset in the dataset dictionary\n",
      " |      As :func:`datasets.Dataset.set_format`, this can be reset using :func:`datasets.Dataset.reset_format`\n",
      " |      \n",
      " |      Args:\n",
      " |          transform (:obj:`Callable`, optional): user-defined formatting transform, replaces the format defined by :func:`datasets.Dataset.set_format`\n",
      " |              A formatting function is a callable that takes a batch (as a dict) as input and returns a batch.\n",
      " |              This function is applied right before returning the objects in ``__getitem__``.\n",
      " |          columns (:obj:`List[str]`, optional): columns to format in the output\n",
      " |              If specified, then the input batch of the transform only contains those columns.\n",
      " |          output_all_columns (:obj:`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n",
      " |              If set to True, then the other un-formatted columns are kept with the output of the transform.\n",
      " |  \n",
      " |  shuffle(self, seeds: Union[int, Dict[str, Optional[int]], NoneType] = None, seed: Optional[int] = None, generators: Optional[Dict[str, numpy.random._generator.Generator]] = None, keep_in_memory: bool = False, load_from_cache_file: bool = True, indices_cache_file_names: Optional[Dict[str, Optional[str]]] = None, writer_batch_size: Optional[int] = 1000) -> 'DatasetDict'\n",
      " |      Create a new Dataset where the rows are shuffled.\n",
      " |      \n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Currently shuffling uses numpy random generators.\n",
      " |      You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPy's default random generator (PCG64).\n",
      " |      \n",
      " |      Args:\n",
      " |          seeds (`Dict[str, int]` or `int`, optional): A seed to initialize the default BitGenerator if ``generator=None``.\n",
      " |              If None, then fresh, unpredictable entropy will be pulled from the OS.\n",
      " |              If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n",
      " |              You can provide one :obj:`seed` per dataset in the dataset dictionary.\n",
      " |          seed (Optional `int`): A seed to initialize the default BitGenerator if ``generator=None``. Alias for seeds (a `ValueError` is raised if both are provided).\n",
      " |          generators (Optional `Dict[str, np.random.Generator]`): Numpy random Generator to use to compute the permutation of the dataset rows.\n",
      " |              If ``generator=None`` (default), uses np.random.default_rng (the default BitGenerator (PCG64) of NumPy).\n",
      " |              You have to provide one :obj:`generator` per dataset in the dataset dictionary.\n",
      " |          keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          indices_cache_file_names (`Dict[str, str]`, optional): Provide the name of a path for the cache file. It is used to store the\n",
      " |              indices mappings instead of the automatically generated cache file name.\n",
      " |              You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
      " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> ds[\"train\"][\"label\"][:10]\n",
      " |      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " |      \n",
      " |      # set a seed\n",
      " |      >>> shuffled_ds = ds.shuffle(seed=42)\n",
      " |      >>> shuffled_ds[\"train\"][\"label\"][:10]\n",
      " |      [0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      " |      ```\n",
      " |  \n",
      " |  sort(self, column: str, reverse: bool = False, kind: str = None, null_placement: str = 'last', keep_in_memory: bool = False, load_from_cache_file: bool = True, indices_cache_file_names: Optional[Dict[str, Optional[str]]] = None, writer_batch_size: Optional[int] = 1000) -> 'DatasetDict'\n",
      " |      Create a new dataset sorted according to a column.\n",
      " |      The transformation is applied to all the datasets of the dataset dictionary.\n",
      " |      \n",
      " |      Currently sorting according to a column name uses pandas sorting algorithm under the hood.\n",
      " |      The column should thus be a pandas compatible type (in particular not a nested type).\n",
      " |      This also means that the column used for sorting is fully loaded in memory (which should be fine in most cases).\n",
      " |      \n",
      " |      Args:\n",
      " |          column (:obj:`str`): column name to sort by.\n",
      " |          reverse (:obj:`bool`, default `False`): If True, sort by descending order rather then ascending.\n",
      " |          kind (:obj:`str`, optional): Pandas algorithm for sorting selected in {‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’},\n",
      " |              The default is ‘quicksort’. Note that both ‘stable’ and ‘mergesort’ use timsort under the covers and, in general,\n",
      " |              the actual implementation will vary with data type. The ‘mergesort’ option is retained for backwards compatibility.\n",
      " |          null_placement (:obj:`str`, default `last`):\n",
      " |              Put `None` values at the beginning if ‘first‘; ‘last‘ puts `None` values at the end.\n",
      " |      \n",
      " |              *New in version 1.14.2*\n",
      " |          keep_in_memory (:obj:`bool`, default `False`): Keep the sorted indices in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (:obj:`bool`, default `True`): If a cache file storing the sorted indices\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          indices_cache_file_names (`Optional[Dict[str, str]]`, defaults to `None`): Provide the name of a path for the cache file. It is used to store the\n",
      " |              indices mapping instead of the automatically generated cache file name.\n",
      " |              You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
      " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> ds[\"train\"][\"label\"][:10]\n",
      " |      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " |      >>> sorted_ds = ds.sort(\"label\")\n",
      " |      >>> sorted_ds[\"train\"][\"label\"][:10]\n",
      " |      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " |      ```\n",
      " |  \n",
      " |  unique(self, column: str) -> Dict[str, List]\n",
      " |      Return a list of the unique elements in a column for each split.\n",
      " |      \n",
      " |      This is implemented in the low-level backend and as such, very fast.\n",
      " |      \n",
      " |      Args:\n",
      " |          column (:obj:`str`):\n",
      " |              column name (list all the column names with :func:`datasets.Dataset.column_names`)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Dict[:obj:`str`, :obj:`list`]: Dictionary of unique elements in the given column.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> ds.unique(\"label\")\n",
      " |      {'test': [1, 0], 'train': [1, 0], 'validation': [1, 0]}\n",
      " |      ```\n",
      " |  \n",
      " |  with_format(self, type: Optional[str] = None, columns: Optional[List] = None, output_all_columns: bool = False, **format_kwargs) -> 'DatasetDict'\n",
      " |      Set ``__getitem__`` return format (type and columns). The data formatting is applied on-the-fly.\n",
      " |      The format ``type`` (for example \"numpy\") is used to format batches when using ``__getitem__``.\n",
      " |      The format is set for every dataset in the dataset dictionary\n",
      " |      \n",
      " |      It's also possible to use custom transforms for formatting using :func:`datasets.Dataset.with_transform`.\n",
      " |      \n",
      " |      Contrary to :func:`datasets.DatasetDict.set_format`, ``with_format`` returns a new DatasetDict object with new Dataset objects.\n",
      " |      \n",
      " |      Args:\n",
      " |          type (:obj:`str`, optional):\n",
      " |              Either output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow'].\n",
      " |              None means ``__getitem__`` returns python objects (default)\n",
      " |          columns (:obj:`List[str]`, optional): columns to format in the output\n",
      " |              None means ``__getitem__`` returns all columns (default)\n",
      " |          output_all_columns (:obj:`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n",
      " |          **format_kwargs (additional keyword arguments): keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
      " |      >>> ds[\"train\"].format\n",
      " |      {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': None}\n",
      " |      >>> ds = ds.with_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      " |      >>> ds[\"train\"].format\n",
      " |      {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': 'tensorflow'}\n",
      " |      ```\n",
      " |  \n",
      " |  with_transform(self, transform: Optional[Callable], columns: Optional[List] = None, output_all_columns: bool = False) -> 'DatasetDict'\n",
      " |      Set ``__getitem__`` return format using this transform. The transform is applied on-the-fly on batches when ``__getitem__`` is called.\n",
      " |      The transform is set for every dataset in the dataset dictionary\n",
      " |      \n",
      " |      As :func:`datasets.Dataset.set_format`, this can be reset using :func:`datasets.Dataset.reset_format`.\n",
      " |      \n",
      " |      Contrary to :func:`datasets.DatasetDict.set_transform`, ``with_transform`` returns a new DatasetDict object with new Dataset objects.\n",
      " |      \n",
      " |      Args:\n",
      " |          transform (:obj:`Callable`, optional): user-defined formatting transform, replaces the format defined by :func:`datasets.Dataset.set_format`\n",
      " |              A formatting function is a callable that takes a batch (as a dict) as input and returns a batch.\n",
      " |              This function is applied right before returning the objects in ``__getitem__``.\n",
      " |          columns (:obj:`List[str]`, optional): columns to format in the output\n",
      " |              If specified, then the input batch of the transform only contains those columns.\n",
      " |          output_all_columns (:obj:`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n",
      " |              If set to True, then the other un-formatted columns are kept with the output of the transform.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      >>> def encode(example):\n",
      " |      ...     return tokenizer(example['text'], truncation=True, padding=True, return_tensors=\"pt\")\n",
      " |      >>> ds = ds.with_transform(encode)\n",
      " |      >>> ds[\"train\"][0]\n",
      " |      {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      " |       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      " |       1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
      " |       'input_ids': tensor([  101,  1103,  2067,  1110, 17348,  1106,  1129,  1103,  6880,  1432,\n",
      " |              112,   188,  1207,   107, 14255,  1389,   107,  1105,  1115,  1119,\n",
      " |              112,   188,  1280,  1106,  1294,   170, 24194,  1256,  3407,  1190,\n",
      " |              170, 11791,  5253,   188,  1732,  7200, 10947, 12606,  2895,   117,\n",
      " |              179,  7766,   118,   172, 15554,  1181,  3498,  6961,  3263,  1137,\n",
      " |              188,  1566,  7912, 14516,  6997,   119,   102]),\n",
      " |       'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      " |              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      " |              0, 0, 0, 0, 0, 0, 0, 0, 0])}\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_csv(path_or_paths: Dict[str, Union[str, bytes, os.PathLike]], features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, **kwargs) -> 'DatasetDict'\n",
      " |      Create DatasetDict from CSV file(s).\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_paths (dict of path-like): Path(s) of the CSV file(s).\n",
      " |          features (:class:`Features`, optional): Dataset features.\n",
      " |          cache_dir (str, optional, default=\"~/.cache/huggingface/datasets\"): Directory to cache data.\n",
      " |          keep_in_memory (bool, default=False): Whether to copy the data in-memory.\n",
      " |          **kwargs (additional keyword arguments): Keyword arguments to be passed to :meth:`pandas.read_csv`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`DatasetDict`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import DatasetDict\n",
      " |      >>> ds = DatasetDict.from_csv({'train': 'path/to/dataset.csv'})\n",
      " |      ```\n",
      " |  \n",
      " |  from_json(path_or_paths: Dict[str, Union[str, bytes, os.PathLike]], features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, **kwargs) -> 'DatasetDict'\n",
      " |      Create DatasetDict from JSON Lines file(s).\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_paths (path-like or list of path-like): Path(s) of the JSON Lines file(s).\n",
      " |          features (:class:`Features`, optional): Dataset features.\n",
      " |          cache_dir (str, optional, default=\"~/.cache/huggingface/datasets\"): Directory to cache data.\n",
      " |          keep_in_memory (bool, default=False): Whether to copy the data in-memory.\n",
      " |          **kwargs (additional keyword arguments): Keyword arguments to be passed to :class:`JsonConfig`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`DatasetDict`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import DatasetDict\n",
      " |      >>> ds = DatasetDict.from_json({'train': 'path/to/dataset.json'})\n",
      " |      ```\n",
      " |  \n",
      " |  from_parquet(path_or_paths: Dict[str, Union[str, bytes, os.PathLike]], features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, columns: Optional[List[str]] = None, **kwargs) -> 'DatasetDict'\n",
      " |      Create DatasetDict from Parquet file(s).\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_paths (dict of path-like): Path(s) of the CSV file(s).\n",
      " |          features (:class:`Features`, optional): Dataset features.\n",
      " |          cache_dir (str, optional, default=\"~/.cache/huggingface/datasets\"): Directory to cache data.\n",
      " |          keep_in_memory (bool, default=False): Whether to copy the data in-memory.\n",
      " |          columns (:obj:`List[str]`, optional): If not None, only these columns will be read from the file.\n",
      " |              A column name may be a prefix of a nested field, e.g. 'a' will select\n",
      " |              'a.b', 'a.c', and 'a.d.e'.\n",
      " |          **kwargs (additional keyword arguments): Keyword arguments to be passed to :class:`ParquetConfig`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`DatasetDict`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import DatasetDict\n",
      " |      >>> ds = DatasetDict.from_parquet({'train': 'path/to/dataset/parquet'})\n",
      " |      ```\n",
      " |  \n",
      " |  from_text(path_or_paths: Dict[str, Union[str, bytes, os.PathLike]], features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, **kwargs) -> 'DatasetDict'\n",
      " |      Create DatasetDict from text file(s).\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_paths (dict of path-like): Path(s) of the text file(s).\n",
      " |          features (:class:`Features`, optional): Dataset features.\n",
      " |          cache_dir (str, optional, default=\"~/.cache/huggingface/datasets\"): Directory to cache data.\n",
      " |          keep_in_memory (bool, default=False): Whether to copy the data in-memory.\n",
      " |          **kwargs (additional keyword arguments): Keyword arguments to be passed to :class:`TextConfig`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`DatasetDict`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import DatasetDict\n",
      " |      >>> ds = DatasetDict.from_text({'train': 'path/to/dataset.txt'})\n",
      " |      ```\n",
      " |  \n",
      " |  load_from_disk(dataset_dict_path: str, fs=None, keep_in_memory: Optional[bool] = None) -> 'DatasetDict'\n",
      " |      Load a dataset that was previously saved using :meth:`save_to_disk` from a filesystem using either\n",
      " |      :class:`~filesystems.S3FileSystem` or ``fsspec.spec.AbstractFileSystem``.\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset_dict_path (:obj:`str`): Path (e.g. ``\"dataset/train\"``) or remote URI (e.g.\n",
      " |              ``\"s3//my-bucket/dataset/train\"``) of the dataset dict directory where the dataset dict will be loaded\n",
      " |              from.\n",
      " |          fs (:class:`~filesystems.S3FileSystem` or ``fsspec.spec.AbstractFileSystem``, optional, default ``None``):\n",
      " |              Instance of the remote filesystem used to download the files from.\n",
      " |          keep_in_memory (:obj:`bool`, default ``None``): Whether to copy the dataset in-memory. If `None`, the\n",
      " |              dataset will not be copied in-memory unless explicitly enabled by setting\n",
      " |              `datasets.config.IN_MEMORY_MAX_SIZE` to nonzero. See more details in the\n",
      " |              :ref:`load_dataset_enhancing_performance` section.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`DatasetDict`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds = load_from_disk('path/to/dataset/directory')\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  cache_files\n",
      " |      The cache files containing the Apache Arrow table backing each split.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> ds.cache_files\n",
      " |      {'test': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-test.arrow'}],\n",
      " |       'train': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-train.arrow'}],\n",
      " |       'validation': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-validation.arrow'}]}\n",
      " |      ```\n",
      " |  \n",
      " |  column_names\n",
      " |      Names of the columns in each split of the dataset.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> ds.column_names\n",
      " |      {'test': ['text', 'label'],\n",
      " |       'train': ['text', 'label'],\n",
      " |       'validation': ['text', 'label']}\n",
      " |      ```\n",
      " |  \n",
      " |  data\n",
      " |      The Apache Arrow tables backing each split.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> ds.data\n",
      " |      ```\n",
      " |  \n",
      " |  num_columns\n",
      " |      Number of columns in each split of the dataset.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> ds.num_columns\n",
      " |      {'test': 2, 'train': 2, 'validation': 2}\n",
      " |      ```\n",
      " |  \n",
      " |  num_rows\n",
      " |      Number of rows in each split of the dataset (same as :func:`datasets.Dataset.__len__`).\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> ds.num_rows\n",
      " |      {'test': 1066, 'train': 8530, 'validation': 1066}\n",
      " |      ```\n",
      " |  \n",
      " |  shape\n",
      " |      Shape of each split of the dataset (number of columns, number of rows).\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\")\n",
      " |      >>> ds.shape\n",
      " |      {'test': (1066, 2), 'train': (8530, 2), 'validation': (1066, 2)}\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if the dictionary has the specified key, else False.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __ior__(self, value, /)\n",
      " |      Return self|=value.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __or__(self, value, /)\n",
      " |      Return self|value.\n",
      " |  \n",
      " |  __reversed__(self, /)\n",
      " |      Return a reverse iterator over the dict keys.\n",
      " |  \n",
      " |  __ror__(self, value, /)\n",
      " |      Return value|self.\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  copy(...)\n",
      " |      D.copy() -> a shallow copy of D\n",
      " |  \n",
      " |  get(self, key, default=None, /)\n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      \n",
      " |      If key is not found, default is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(self, /)\n",
      " |      Remove and return a (key, value) pair as a 2-tuple.\n",
      " |      \n",
      " |      Pairs are returned in LIFO (last-in, first-out) order.\n",
      " |      Raises KeyError if the dict is empty.\n",
      " |  \n",
      " |  setdefault(self, key, default=None, /)\n",
      " |      Insert key with a value of default if key is not in the dictionary.\n",
      " |      \n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  update(...)\n",
      " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __class_getitem__(...) from builtins.type\n",
      " |      See PEP 585\n",
      " |  \n",
      " |  fromkeys(iterable, value=None, /) from builtins.type\n",
      " |      Create a new dictionary with keys from iterable and values set to value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from builtins.dict:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DatasetDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5459379c-cb87-4f8b-bdc9-0172af265ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "panx_ch = defaultdict(DatasetDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "281aa30d-fa47-41fc-8cdf-eaad52821abb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset xtreme (/home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3ffac3899042e79ef33fae8985e7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-ad1b311e95818edf.arrow\n",
      "Loading cached shuffled indices for dataset at /home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-9e4b5e384626785e.arrow\n",
      "Loading cached shuffled indices for dataset at /home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-b80ca41f318cd7e7.arrow\n",
      "Reusing dataset xtreme (/home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2f97f3e4714453ac3e121ccbacfe29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-222f2a739e50779b.arrow\n",
      "Loading cached shuffled indices for dataset at /home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-ae79577dfb0e7498.arrow\n",
      "Loading cached shuffled indices for dataset at /home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-0bc206f54324de18.arrow\n",
      "Reusing dataset xtreme (/home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315c2ce0d1f249f99dee6153963cddba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-2a286f85a785394c.arrow\n",
      "Loading cached shuffled indices for dataset at /home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-55894a1d8ab171ae.arrow\n",
      "Loading cached shuffled indices for dataset at /home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-b2a9c20bbec1f943.arrow\n",
      "Reusing dataset xtreme (/home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b76842c5a294a8ab92a949a56b5efb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-b91db9df81081a1f.arrow\n",
      "Loading cached shuffled indices for dataset at /home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-dea8d95ed2e6a82a.arrow\n",
      "Loading cached shuffled indices for dataset at /home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-56dba7892a30e39c.arrow\n"
     ]
    }
   ],
   "source": [
    "for lang, frac in zip(langs, fracs):\n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split] = (ds[split].shuffle(seed=0).select(range(int(frac * ds[split].num_rows))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fa324c7-b7ae-43dd-a385-bde34175bfde",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(datasets.dataset_dict.DatasetDict,\n",
       "            {'de': DatasetDict({\n",
       "                 train: Dataset({\n",
       "                     features: ['tokens', 'ner_tags', 'langs'],\n",
       "                     num_rows: 2000\n",
       "                 })\n",
       "                 validation: Dataset({\n",
       "                     features: ['tokens', 'ner_tags', 'langs'],\n",
       "                     num_rows: 1000\n",
       "                 })\n",
       "                 test: Dataset({\n",
       "                     features: ['tokens', 'ner_tags', 'langs'],\n",
       "                     num_rows: 1000\n",
       "                 })\n",
       "             }),\n",
       "             'fr': DatasetDict({\n",
       "                 train: Dataset({\n",
       "                     features: ['tokens', 'ner_tags', 'langs'],\n",
       "                     num_rows: 2000\n",
       "                 })\n",
       "                 validation: Dataset({\n",
       "                     features: ['tokens', 'ner_tags', 'langs'],\n",
       "                     num_rows: 1000\n",
       "                 })\n",
       "                 test: Dataset({\n",
       "                     features: ['tokens', 'ner_tags', 'langs'],\n",
       "                     num_rows: 1000\n",
       "                 })\n",
       "             }),\n",
       "             'it': DatasetDict({\n",
       "                 train: Dataset({\n",
       "                     features: ['tokens', 'ner_tags', 'langs'],\n",
       "                     num_rows: 2000\n",
       "                 })\n",
       "                 validation: Dataset({\n",
       "                     features: ['tokens', 'ner_tags', 'langs'],\n",
       "                     num_rows: 1000\n",
       "                 })\n",
       "                 test: Dataset({\n",
       "                     features: ['tokens', 'ner_tags', 'langs'],\n",
       "                     num_rows: 1000\n",
       "                 })\n",
       "             }),\n",
       "             'en': DatasetDict({\n",
       "                 train: Dataset({\n",
       "                     features: ['tokens', 'ner_tags', 'langs'],\n",
       "                     num_rows: 2000\n",
       "                 })\n",
       "                 validation: Dataset({\n",
       "                     features: ['tokens', 'ner_tags', 'langs'],\n",
       "                     num_rows: 1000\n",
       "                 })\n",
       "                 test: Dataset({\n",
       "                     features: ['tokens', 'ner_tags', 'langs'],\n",
       "                     num_rows: 1000\n",
       "                 })\n",
       "             })})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "927c2b54-9854-4fd5-ae7c-221dae7dff70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "{'train': 2000, 'validation': 1000, 'test': 1000}\n",
      "fr\n",
      "{'train': 2000, 'validation': 1000, 'test': 1000}\n",
      "it\n",
      "{'train': 2000, 'validation': 1000, 'test': 1000}\n",
      "en\n",
      "{'train': 2000, 'validation': 1000, 'test': 1000}\n"
     ]
    }
   ],
   "source": [
    "for lang in langs:\n",
    "    print(lang)\n",
    "    print(panx_ch[lang].num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b8f0a9d-4b7e-4adc-8abc-4ccfad0ba726",
   "metadata": {},
   "outputs": [],
   "source": [
    "element = panx_ch[\"de\"][\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03adb9e1-a7f3-4717-a63f-12360e56309d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens\n",
      "['2.000', 'Einwohnern', 'an', 'der', 'Danziger', 'Bucht', 'in', 'der', 'polnischen', 'Woiwodschaft', 'Pommern', '.']\n",
      "ner_tags\n",
      "[0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\n",
      "langs\n",
      "['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']\n"
     ]
    }
   ],
   "source": [
    "for key, value in element.items():\n",
    "    print(key)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b2b6f71-f3c6-4d05-b56f-2f898ed8db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40539df1-1df0-4efe-8e97-0804f2f504e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.features.features.ClassLabel"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98961853-e762-426a-adc2-f6f6e77dc646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_names(batch):\n",
    "    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1760b33c-dab9-4131-852b-461aa14471f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function create_tag_names at 0x7f3b89c8b310> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57dab219628d4960a6da9e93438df16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b4953900d04dbd8d30703307b35973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f833f6861f140dda5d40a0682b49644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "panx_de = panx_ch[\"de\"].map(create_tag_names, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3346ac01-a355-4282-a5f1-95257828dbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>2.000</td>\n",
       "      <td>Einwohnern</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Danziger</td>\n",
       "      <td>Bucht</td>\n",
       "      <td>in</td>\n",
       "      <td>der</td>\n",
       "      <td>polnischen</td>\n",
       "      <td>Woiwodschaft</td>\n",
       "      <td>Pommern</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0           1   2    3         4      5   6    7           8   \\\n",
       "Tokens  2.000  Einwohnern  an  der  Danziger  Bucht  in  der  polnischen   \n",
       "Tags        O           O   O    O     B-LOC  I-LOC   O    O       B-LOC   \n",
       "\n",
       "                  9        10 11  \n",
       "Tokens  Woiwodschaft  Pommern  .  \n",
       "Tags           B-LOC    I-LOC  O  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_example = panx_de[\"train\"][0]\n",
    "pd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]], ['Tokens', 'Tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "591aa6f0-2b32-466a-a116-c74b630d6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in panx_de.items():\n",
    "    for row in dataset[\"ner_tags_str\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8584c95f-5947-4e19-8d80-2504f2d59d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'train': Counter({'LOC': 970, 'ORG': 850, 'PER': 965}),\n",
       "             'validation': Counter({'ORG': 409, 'LOC': 487, 'PER': 497}),\n",
       "             'test': Counter({'LOC': 489, 'PER': 505, 'ORG': 431})})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split2freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec966fb2-5590-4191-a0a9-a0dc204dfae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>970</td>\n",
       "      <td>850</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>487</td>\n",
       "      <td>409</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>489</td>\n",
       "      <td>431</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            LOC  ORG  PER\n",
       "train       970  850  965\n",
       "validation  487  409  497\n",
       "test        489  431  505"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(split2freqs, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb2135-22fc-4355-b74e-374a1bc08ee9",
   "metadata": {},
   "source": [
    "## Multilingual Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f3eb19-176f-4941-a02c-46432a55ce3d",
   "metadata": {},
   "source": [
    "A Closer Look at Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59607925-ca49-46bc-bc48-e90970590875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c09b523-ad23-4cd4-95c0-c275f8b6505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"bert-base-cased\"\n",
    "xlmr_model_name = \"xlm-roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93384bea-0973-41db-a4cc-b7e45f06d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb5bc475-6a35-41df-b06d-77796b267c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Jack Sparrow loves New York!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0118a34-c417-4a8a-9e99-ea0f9fb2217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca2e0099-b900-4027-b37d-0e14963abea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'Jack', 'Spa', '##rrow', 'loves', 'New', 'York', '!', '[SEP]']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68cde649-f7c5-4ddf-b30d-8f3d91f8ef8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '▁Jack', '▁Spar', 'row', '▁love', 's', '▁New', '▁York', '!', '</s>']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlmr_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64f8543f-6cea-4e9f-b82a-9e1765251b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Jack Sparrow loves New York!</s>'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(xlmr_tokens).replace(u\"\\u2581\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d78124-0d02-4eca-b28b-78854a7d5399",
   "metadata": {},
   "source": [
    "## Transformers for Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d7db762-405e-4ae5-84bb-420a233d9a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "069dee7d-9b4c-456c-97d1-3014b90a746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaConfig\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # Loan model body\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # Set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # Load and initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, \n",
    "                token_type_ids=None, labels=None, **kwargs):\n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "                              token_type_ids=token_type_ids, **kwargs)\n",
    "        # Apply classifier to encoder representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        # Return model output object\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits,\n",
    "                                    hidden_states=outputs.hidden_states,\n",
    "                                    attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8adca2a-3ab4-4fab-8c5a-716d38a1da88",
   "metadata": {},
   "source": [
    "### Loading a Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a154a723-867a-47b5-b299-d351bfb9fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0bdabc19-f9f2-41b9-b69a-a0ce14a7da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0c0513b-46f0-4ad9-8e01-c1ff60d40314",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n",
    "                                         num_attention_heads=1,\n",
    "                                         num_hidden_layers=1,\n",
    "                                         num_labels=tags.num_classes,\n",
    "                                         id2label=index2tag, \n",
    "                                         label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15ac7c66-e9ab-4c73-9761-74e0e47ed0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaConfig {\n",
       "  \"_name_or_path\": \"xlm-roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"XLMRobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"O\",\n",
       "    \"1\": \"B-PER\",\n",
       "    \"2\": \"I-PER\",\n",
       "    \"3\": \"B-ORG\",\n",
       "    \"4\": \"I-ORG\",\n",
       "    \"5\": \"B-LOC\",\n",
       "    \"6\": \"I-LOC\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"B-LOC\": 5,\n",
       "    \"B-ORG\": 3,\n",
       "    \"B-PER\": 1,\n",
       "    \"I-LOC\": 6,\n",
       "    \"I-ORG\": 4,\n",
       "    \"I-PER\": 2,\n",
       "    \"O\": 0\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"xlm-roberta\",\n",
       "  \"num_attention_heads\": 1,\n",
       "  \"num_hidden_layers\": 1,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.19.4\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 250002\n",
       "}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlmr_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "678c592c-b78c-45e6-b6df-e5ee9c5d59b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'lm_head.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'lm_head.dense.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "xlmr_model = (XLMRobertaForTokenClassification\n",
    "              .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "              .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cbdf8564-aaa9-40c5-ada4-4f00615441ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jack Sparrow loves New York!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e92e13a6-b75e-4f5d-90cc-f99d79fe81df",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0b81f326-d8b2-4c78-9aa9-fa0d1d8c4390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁York</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input IDs</th>\n",
       "      <td>0</td>\n",
       "      <td>21763</td>\n",
       "      <td>37456</td>\n",
       "      <td>15555</td>\n",
       "      <td>5161</td>\n",
       "      <td>7</td>\n",
       "      <td>2356</td>\n",
       "      <td>5753</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0      1      2      3      4  5     6      7   8     9\n",
       "Tokens     <s>  ▁Jack  ▁Spar    row  ▁love  s  ▁New  ▁York   !  </s>\n",
       "Input IDs    0  21763  37456  15555   5161  7  2356   5753  38     2"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dea529c9-9899-4924-9979-9adafdb5636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = xlmr_model(input_ids.to(device)).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7b98d3c8-b1c5-4a92-982e-01ea2e43856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.argmax(outputs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "48203d8e-4dbc-4de6-948c-3cbf667bdf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in sequence: 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3f5cfbdc-fd29-40d9-a7be-a56048021e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of outputs: torch.Size([1, 10, 7])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of outputs: {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f224d4ae-62eb-46ff-98f4-704212072461",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [tags.names[p] for p in predictions[0].cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0963c888-b7d2-4eef-8546-86f0fedd2e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁York</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2      3      4      5      6      7      8      9\n",
       "Tokens    <s>  ▁Jack  ▁Spar    row  ▁love      s   ▁New  ▁York      !   </s>\n",
       "Tags    B-ORG  I-ORG  B-ORG  I-ORG  I-ORG  I-ORG  B-PER  I-ORG  I-ORG  B-LOC"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7048db32-9389-4740-9969-f206cca64220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    \n",
    "    tokens = tokenizer(text).tokens()\n",
    "    \n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    outputs = model(input_ids)[0]\n",
    "    \n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    \n",
    "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "    \n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ed5fd98d-855a-49af-8bd9-89d3f565a153",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d92a1823-7dc8-41fb-9d4e-8f7fb04edf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = xlmr_tokenizer(words, is_split_into_words=True)\n",
    "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2dafc2e7-3b75-45a0-9553-dba7b4be08b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 70101, 176581, 19, 142, 122, 2290, 708, 1505, 18363, 18, 23, 122, 127474, 15439, 13787, 14, 15263, 18917, 663, 6947, 19, 6, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "159fce05-c373-47f9-8d27-5866a8005ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "90689e35-3874-40c7-9faf-8a6e0125b2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁2.000</td>\n",
       "      <td>▁Einwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>▁an</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁Dan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>▁Buch</td>\n",
       "      <td>...</td>\n",
       "      <td>▁Wo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>▁Po</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1           2  3    4     5     6   7    8      9   ...   15  \\\n",
       "Tokens  <s>  ▁2.000  ▁Einwohner  n  ▁an  ▁der  ▁Dan  zi  ger  ▁Buch  ...  ▁Wo   \n",
       "\n",
       "       16   17      18   19    20 21 22 23    24  \n",
       "Tokens  i  wod  schaft  ▁Po  mmer  n  ▁  .  </s>  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([tokens], index=[\"Tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "462b08d9-9981-431e-9a36-ea7edff65366",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = tokenized_input.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4f13b6e7-46e7-48a2-b1d2-f12750f2afdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁2.000</td>\n",
       "      <td>▁Einwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>▁an</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁Dan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>▁Buch</td>\n",
       "      <td>...</td>\n",
       "      <td>▁Wo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>▁Po</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1           2  3    4     5     6   7    8      9   ...  \\\n",
       "Tokens     <s>  ▁2.000  ▁Einwohner  n  ▁an  ▁der  ▁Dan  zi  ger  ▁Buch  ...   \n",
       "Word IDs  None       0           1  1    2     3     4   4    4      5  ...   \n",
       "\n",
       "           15 16   17      18   19    20  21  22  23    24  \n",
       "Tokens    ▁Wo  i  wod  schaft  ▁Po  mmer   n   ▁   .  </s>  \n",
       "Word IDs    9  9    9       9   10    10  10  11  11  None  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "afdef318-f143-4735-b3fa-9c4f755b8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_word_idx = None\n",
    "label_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1772889c-2de8-4b78-ac47-7e48d03780a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == previous_word_idx:\n",
    "        label_ids.append(-100)\n",
    "    elif word_idx != previous_word_idx:\n",
    "        label_ids.append(labels[word_idx])\n",
    "    \n",
    "    previous_word_idx = word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "640503d4-e6dc-471f-978c-5c8768c643e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\n",
    "index = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "35a0acaa-b541-4fbf-909b-c0e3185dd35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁2.000</td>\n",
       "      <td>▁Einwohner</td>\n",
       "      <td>n</td>\n",
       "      <td>▁an</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁Dan</td>\n",
       "      <td>zi</td>\n",
       "      <td>ger</td>\n",
       "      <td>▁Buch</td>\n",
       "      <td>...</td>\n",
       "      <td>▁Wo</td>\n",
       "      <td>i</td>\n",
       "      <td>wod</td>\n",
       "      <td>schaft</td>\n",
       "      <td>▁Po</td>\n",
       "      <td>mmer</td>\n",
       "      <td>n</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label IDs</th>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>6</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labels</th>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>...</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0       1           2     3    4     5      6     7     8   \\\n",
       "Tokens      <s>  ▁2.000  ▁Einwohner     n  ▁an  ▁der   ▁Dan    zi   ger   \n",
       "Word IDs   None       0           1     1    2     3      4     4     4   \n",
       "Label IDs  -100       0           0  -100    0     0      5  -100  -100   \n",
       "Labels      IGN       O           O   IGN    O     O  B-LOC   IGN   IGN   \n",
       "\n",
       "              9   ...     15    16    17      18     19    20    21  22    23  \\\n",
       "Tokens     ▁Buch  ...    ▁Wo     i   wod  schaft    ▁Po  mmer     n   ▁     .   \n",
       "Word IDs       5  ...      9     9     9       9     10    10    10  11    11   \n",
       "Label IDs      6  ...      5  -100  -100    -100      6  -100  -100   0  -100   \n",
       "Labels     I-LOC  ...  B-LOC   IGN   IGN     IGN  I-LOC   IGN   IGN   O   IGN   \n",
       "\n",
       "             24  \n",
       "Tokens     </s>  \n",
       "Word IDs   None  \n",
       "Label IDs  -100  \n",
       "Labels      IGN  \n",
       "\n",
       "[4 rows x 25 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([tokens, word_ids, label_ids, labels], index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ea910468-bdf4-4a16-b485-e92da8db4318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True,\n",
    "                                     is_split_into_words=True)\n",
    "    labels = []\n",
    "    \n",
    "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
    "        \n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d897f2f9-3914-4161-b6c4-87423c946231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True,\n",
    "                     remove_columns=['langs', 'ner_tags', 'tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d145a37c-4dc8-469f-b4e8-4aaf962f5e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1b5e075f534b0cb8a7543b62f60cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e594685153ce4b3eb3c96742c429658a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269139f1c4f547f7bac7a7f332aa7dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4d3d2ad1-8ee6-472e-8972-914a82523396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f954cba8-409c-4092-999f-b49e7abc5f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "y_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "          [\"B-PER\", \"I-PER\", \"O\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d8deb022-55da-46b2-8c68-b87b313dee73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       0.00      0.00      0.00         1\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         2\n",
      "   macro avg       0.50      0.50      0.50         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "79f7dcbc-fffc-4be6-9687-5f6b307a3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4a41c3a9-686e-4c88-8a17-3923a78cdc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "    \n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "        \n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "        \n",
    "    return preds_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "575bd676-8f25-44c7-bed6-f58f324854aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['I-ORG', 'B-ORG', 'I-ORG', 'I-ORG', 'B-PER', 'B-LOC']],\n",
       " [['O', 'O', 'O', 'O', 'B-LOC', 'I-LOC']])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_predictions(outputs.detach().numpy(), np.array([label_ids]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf733386-6d0c-4c0f-8f87-5192f014c036",
   "metadata": {},
   "source": [
    "## Fine-Tuning XLM-RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b09dd16b-0619-41ce-bf01-05d3d69ff2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cce749ed-d49d-4a2c-8110-c3a90644b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 24\n",
    "logging_steps = len(panx_de_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"small_{xlmr_model_name}-finetuned-panx-de\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7fe5083e-8b8d-48c6-bd2a-8a01df04b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = model_name,\n",
    "    log_level = \"error\",\n",
    "    num_train_epochs = num_epochs,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_steps=1e6,\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps = logging_steps,\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3361de56-9195-4876-a2d6-658ee92a016b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=0,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=IntervalStrategy.EPOCH,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_min_num_params=0,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=-1,\n",
       "log_level=40,\n",
       "log_level_replica=-1,\n",
       "log_on_each_node=True,\n",
       "logging_dir=small_xlm-roberta-base-finetuned-panx-de/runs/Jul15_14-42-13_small-instance,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=83,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1,\n",
       "optim=OptimizerNames.ADAMW_HF,\n",
       "output_dir=small_xlm-roberta-base-finetuned-panx-de,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=24,\n",
       "per_device_train_batch_size=24,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "remove_unused_columns=True,\n",
       "report_to=['mlflow'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=small_xlm-roberta-base-finetuned-panx-de,\n",
       "save_on_each_node=False,\n",
       "save_steps=1000000.0,\n",
       "save_strategy=IntervalStrategy.STEPS,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_legacy_prediction_loop=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.01,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6e551c60-9476-42c3-a780-3e61fd54e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions,\n",
    "                                      eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6de75e71-c6c8-4a28-9f44-23d6486aad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e5296ee3-980d-41eb-9f48-bbd6a7a47af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "03df23f7-2c4f-4253-a63f-65ce5ba5dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return (XLMRobertaForTokenClassification.from_pretrained(\n",
    "    xlmr_model_name, config=xlmr_config).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f05ce1bf-998d-4463-801f-5c977a218e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5002f83a-1b5a-4517-83f2-c6bb9820b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "212be028-1632-4d4c-b94c-0f4e0c64cc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model_init=model_init,\n",
    "                 args=training_args,\n",
    "                 data_collator=data_collator,\n",
    "                 compute_metrics=compute_metrics,\n",
    "                 train_dataset=panx_de_encoded[\"train\"],\n",
    "                 eval_dataset=panx_de_encoded[\"validation\"],\n",
    "                 tokenizer=xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7abc0ff8-c365-4438-88e5-6bebb9da8afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 05:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.965600</td>\n",
       "      <td>0.758908</td>\n",
       "      <td>0.178571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=84, training_loss=0.9618963308277584, metrics={'train_runtime': 335.5687, 'train_samples_per_second': 5.96, 'train_steps_per_second': 0.25, 'total_flos': 3910875832512.0, 'train_loss': 0.9618963308277584, 'epoch': 1.0})"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "23a42587-b81b-4f8c-b296-d0d5a85d5674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9656</td>\n",
       "      <td>0.758908</td>\n",
       "      <td>0.178571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch  Training Loss  Validation Loss        F1\n",
       "0      1         0.9656         0.758908  0.178571"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide_input\n",
    "df = pd.DataFrame(trainer.state.log_history)[['epoch','loss' ,'eval_loss', 'eval_f1']]\n",
    "df = df.rename(columns={\"epoch\":\"Epoch\",\"loss\": \"Training Loss\", \"eval_loss\": \"Validation Loss\", \"eval_f1\":\"F1\"})\n",
    "df['Epoch'] = df[\"Epoch\"].apply(lambda x: round(x))\n",
    "df['Training Loss'] = df[\"Training Loss\"].ffill()\n",
    "df[['Validation Loss', 'F1']] = df[['Validation Loss', 'F1']].bfill().ffill()\n",
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1f8a5277-15b6-47b5-ad45-658d89dc4f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jeff</td>\n",
       "      <td>▁De</td>\n",
       "      <td>an</td>\n",
       "      <td>▁ist</td>\n",
       "      <td>▁ein</td>\n",
       "      <td>▁Informati</td>\n",
       "      <td>ker</td>\n",
       "      <td>▁bei</td>\n",
       "      <td>▁Google</td>\n",
       "      <td>▁in</td>\n",
       "      <td>▁Kaliforni</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1    2   3     4     5           6    7     8        9    10  \\\n",
       "Tokens  <s>  ▁Jeff  ▁De  an  ▁ist  ▁ein  ▁Informati  ker  ▁bei  ▁Google  ▁in   \n",
       "Tags      O  I-PER    O   O     O     O       I-PER    O     O        O    O   \n",
       "\n",
       "                11  12    13  \n",
       "Tokens  ▁Kaliforni  en  </s>  \n",
       "Tags         I-PER   O     O  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide_output\n",
    "text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\n",
    "tag_text(text_de, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d85006-4e5c-459a-ad30-38c320ce16ec",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "00b56a1c-07b4-472c-97bf-6ba7ad6e8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5ba36111-cf65-4149-8fe6-5cc2ad59d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_with_label(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    \n",
    "    batch = data_collator(features)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        output = trainer.model(input_ids, attention_mask)\n",
    "        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n",
    "    \n",
    "    loss = cross_entropy(output.logits.view(-1, 7),\n",
    "                        labels.view(-1), rediction=\"none\")\n",
    "    \n",
    "    loss = loss.view(len(input_ids), -1).cpu().numpy()\n",
    "    \n",
    "    return {\"loss\": loss, \"predicted_label\": predicted_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6bee685c-2b54-4bcd-8764-55fbe548e3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/andreas/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e/cache-1c80317fa3b1799d.arrow\n"
     ]
    }
   ],
   "source": [
    "# hide_output\n",
    "valid_set = panx_de_encoded[\"validation\"]\n",
    "valid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)\n",
    "df = valid_set.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "56bd8685-71eb-453b-9705-6c04c7fd8abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 10699, 11, 15, 16104, 1388, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 3, -100, 4, 4, 4, -100]</td>\n",
       "      <td>[0.0, 0.011659455, 0.0, 0.01750203, 0.01324592...</td>\n",
       "      <td>[4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 56530, 25216, 30121, 152385, 19229, 83982,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, -100, -100, -100, -100, 3, -100, -10...</td>\n",
       "      <td>[0.0, 0.00020537652, 0.0, 0.0, 0.0, 0.0, 1.246...</td>\n",
       "      <td>[6, 0, 0, 0, 0, 0, 5, 1, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 159093, 165, 38506, 122, 153080, 29088, 57...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 3, -100, -100, 0, -100, 0, ...</td>\n",
       "      <td>[0.0, 0.00021491126, 0.00014065707, 0.00021300...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 16459, 242, 5106, 6, 198715, 5106, 242, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, 0, 5, -100, 0, 0, -100]</td>\n",
       "      <td>[0.0, 0.00018249277, 0.00016127716, 0.00018976...</td>\n",
       "      <td>[0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 11022, 2315, 7418, 1079, 8186, 57242, 97, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 3, ...</td>\n",
       "      <td>[0.0, 0.00019512657, 0.00016044283, 0.00023302...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6285</th>\n",
       "      <td>[0, 10333, 599, 7418, 4180, 72, 3700, 542, 900...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, -10...</td>\n",
       "      <td>[0.0, 0.00015650954, 0.00013076403, 0.00020990...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6286</th>\n",
       "      <td>[0, 15497, 7, 91243, 15, 23924, 96220, 1388, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 5, -100, -100, 6, 6, 6, 6, -100]</td>\n",
       "      <td>[0.0, 0.013764698, 0.0, 0.0, 0.013868398, 0.01...</td>\n",
       "      <td>[6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6287</th>\n",
       "      <td>[0, 1858, 566, 12241, 729, 4598, 89841, 68125,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, -100, 0, -100, -100, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.00015829741, 0.00012194367, 0.00026306...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6288</th>\n",
       "      <td>[0, 132005, 11399, 7, 84974, 168, 34525, 84247...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, -100, 0, 0, 5, 6, 0, 0, -100, -100]</td>\n",
       "      <td>[0.0, 0.00020692592, 0.0013584205, 0.0, 0.0001...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6289</th>\n",
       "      <td>[0, 242, 5106, 223660, 5106, 242, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, 1, 0, 0, -100]</td>\n",
       "      <td>[0.0, 0.00017116989, 0.00020907124, 4.8172803,...</td>\n",
       "      <td>[0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6290 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              input_ids  \\\n",
       "0                    [0, 10699, 11, 15, 16104, 1388, 2]   \n",
       "1     [0, 56530, 25216, 30121, 152385, 19229, 83982,...   \n",
       "2     [0, 159093, 165, 38506, 122, 153080, 29088, 57...   \n",
       "3        [0, 16459, 242, 5106, 6, 198715, 5106, 242, 2]   \n",
       "4     [0, 11022, 2315, 7418, 1079, 8186, 57242, 97, ...   \n",
       "...                                                 ...   \n",
       "6285  [0, 10333, 599, 7418, 4180, 72, 3700, 542, 900...   \n",
       "6286    [0, 15497, 7, 91243, 15, 23924, 96220, 1388, 2]   \n",
       "6287  [0, 1858, 566, 12241, 729, 4598, 89841, 68125,...   \n",
       "6288  [0, 132005, 11399, 7, 84974, 168, 34525, 84247...   \n",
       "6289               [0, 242, 5106, 223660, 5106, 242, 2]   \n",
       "\n",
       "                                         attention_mask  \\\n",
       "0                                 [1, 1, 1, 1, 1, 1, 1]   \n",
       "1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "3                           [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                 ...   \n",
       "6285  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6286                        [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "6287  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6288               [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "6289                              [1, 1, 1, 1, 1, 1, 1]   \n",
       "\n",
       "                                                 labels  \\\n",
       "0                        [-100, 3, -100, 4, 4, 4, -100]   \n",
       "1     [-100, 0, -100, -100, -100, -100, 3, -100, -10...   \n",
       "2     [-100, 0, 0, 0, 0, 3, -100, -100, 0, -100, 0, ...   \n",
       "3                  [-100, 0, 0, 0, 5, -100, 0, 0, -100]   \n",
       "4     [-100, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 3, ...   \n",
       "...                                                 ...   \n",
       "6285  [-100, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, -10...   \n",
       "6286            [-100, 5, -100, -100, 6, 6, 6, 6, -100]   \n",
       "6287  [-100, 0, 0, 0, 0, -100, 0, -100, -100, 0, 0, ...   \n",
       "6288   [-100, 0, 0, -100, 0, 0, 5, 6, 0, 0, -100, -100]   \n",
       "6289                        [-100, 0, 0, 1, 0, 0, -100]   \n",
       "\n",
       "                                                   loss  \\\n",
       "0     [0.0, 0.011659455, 0.0, 0.01750203, 0.01324592...   \n",
       "1     [0.0, 0.00020537652, 0.0, 0.0, 0.0, 0.0, 1.246...   \n",
       "2     [0.0, 0.00021491126, 0.00014065707, 0.00021300...   \n",
       "3     [0.0, 0.00018249277, 0.00016127716, 0.00018976...   \n",
       "4     [0.0, 0.00019512657, 0.00016044283, 0.00023302...   \n",
       "...                                                 ...   \n",
       "6285  [0.0, 0.00015650954, 0.00013076403, 0.00020990...   \n",
       "6286  [0.0, 0.013764698, 0.0, 0.0, 0.013868398, 0.01...   \n",
       "6287  [0.0, 0.00015829741, 0.00012194367, 0.00026306...   \n",
       "6288  [0.0, 0.00020692592, 0.0013584205, 0.0, 0.0001...   \n",
       "6289  [0.0, 0.00017116989, 0.00020907124, 4.8172803,...   \n",
       "\n",
       "                                        predicted_label  \n",
       "0     [4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
       "1     [6, 0, 0, 0, 0, 0, 5, 1, 2, 2, 2, 2, 2, 2, 2, ...  \n",
       "2     [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, ...  \n",
       "...                                                 ...  \n",
       "6285  [0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 0, 0, ...  \n",
       "6286  [6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...  \n",
       "6287  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6288  [0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6289  [0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[6290 rows x 5 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "143755e3-77ef-44b8-91ac-7af7093bb240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "index2tag[-100] = \"IGN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b6394cc6-80a8-473b-b374-0a8d59838b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"input_tokens\"] = df[\"input_ids\"].apply(\n",
    "    lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\n",
    "\n",
    "df[\"predicted_label\"] = df[\"predicted_label\"].apply(\n",
    "    lambda x: [index2tag[i] for i in x])\n",
    "\n",
    "df[\"labels\"] = df[\"labels\"].apply(\n",
    "    lambda x: [index2tag[i] for i in x])\n",
    "\n",
    "df['loss'] = df.apply(\n",
    "    lambda x: x['loss'][:len(x['input_ids'])], axis=1)\n",
    "\n",
    "df['predicted_label'] = df.apply(\n",
    "    lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "46fad0ce-cf30-40f9-9c6f-644fd81c9a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 10699, 11, 15, 16104, 1388, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[IGN, B-ORG, IGN, I-ORG, I-ORG, I-ORG, IGN]</td>\n",
       "      <td>[0.0, 0.011659455, 0.0, 0.01750203, 0.01324592...</td>\n",
       "      <td>[I-ORG, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG]</td>\n",
       "      <td>[&lt;s&gt;, ▁Ham, a, ▁(, ▁Unternehmen, ▁), &lt;/s&gt;]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            input_ids         attention_mask  \\\n",
       "0  [0, 10699, 11, 15, 16104, 1388, 2]  [1, 1, 1, 1, 1, 1, 1]   \n",
       "\n",
       "                                        labels  \\\n",
       "0  [IGN, B-ORG, IGN, I-ORG, I-ORG, I-ORG, IGN]   \n",
       "\n",
       "                                                loss  \\\n",
       "0  [0.0, 0.011659455, 0.0, 0.01750203, 0.01324592...   \n",
       "\n",
       "                                     predicted_label  \\\n",
       "0  [I-ORG, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG]   \n",
       "\n",
       "                                 input_tokens  \n",
       "0  [<s>, ▁Ham, a, ▁(, ▁Unternehmen, ▁), </s>]  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fec51af8-cde0-4a08-80ee-da1f35b575d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>loss</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10699</td>\n",
       "      <td>1</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>0.01</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>▁Ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.02</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>▁(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16104</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.01</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>▁Unternehmen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1388</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.02</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>▁)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56530</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>0.00</td>\n",
       "      <td>O</td>\n",
       "      <td>▁WE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83982</td>\n",
       "      <td>1</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>1.25</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>▁Luz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>1.79</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>▁a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input_ids attention_mask labels  loss predicted_label  input_tokens\n",
       "0     10699              1  B-ORG  0.01           B-ORG          ▁Ham\n",
       "0        15              1  I-ORG  0.02           I-ORG            ▁(\n",
       "0     16104              1  I-ORG  0.01           I-ORG  ▁Unternehmen\n",
       "0      1388              1  I-ORG  0.02           I-ORG            ▁)\n",
       "1     56530              1      O  0.00               O           ▁WE\n",
       "1     83982              1  B-ORG  1.25           B-LOC          ▁Luz\n",
       "1        10              1  I-ORG  1.79           I-PER            ▁a"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide_output\n",
    "df_tokens = df.apply(pd.Series.explode)\n",
    "df_tokens = df_tokens.query(\"labels != 'IGN'\")\n",
    "df_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\n",
    "df_tokens.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "49722127-7bb7-4a25-8dbf-5bd8b45741e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>input_tokens</th>\n",
       "      <td>▁</td>\n",
       "      <td>▁in</td>\n",
       "      <td>▁von</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁und</td>\n",
       "      <td>▁/</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁(</td>\n",
       "      <td>▁)</td>\n",
       "      <td>▁A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6066</td>\n",
       "      <td>989</td>\n",
       "      <td>808</td>\n",
       "      <td>1388</td>\n",
       "      <td>1171</td>\n",
       "      <td>163</td>\n",
       "      <td>2898</td>\n",
       "      <td>246</td>\n",
       "      <td>246</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>228.33</td>\n",
       "      <td>140.17</td>\n",
       "      <td>132.6</td>\n",
       "      <td>122.15</td>\n",
       "      <td>95.71</td>\n",
       "      <td>78.78</td>\n",
       "      <td>76.49</td>\n",
       "      <td>76.08</td>\n",
       "      <td>73.12</td>\n",
       "      <td>58.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0       1      2       3      4      5      6      7  \\\n",
       "input_tokens       ▁     ▁in   ▁von    ▁der   ▁und     ▁/    ▁''     ▁(   \n",
       "count           6066     989    808    1388   1171    163   2898    246   \n",
       "mean            0.04    0.14   0.16    0.09   0.08   0.48   0.03   0.31   \n",
       "sum           228.33  140.17  132.6  122.15  95.71  78.78  76.49  76.08   \n",
       "\n",
       "                  8      9  \n",
       "input_tokens     ▁)     ▁A  \n",
       "count           246    125  \n",
       "mean            0.3   0.47  \n",
       "sum           73.12  58.22  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df_tokens.groupby(\"input_tokens\")[[\"loss\"]]\n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)  # Get rid of multi-level columns\n",
    "    .sort_values(by=\"sum\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(2)\n",
    "    .head(10)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "56ed267a-327a-4b96-8db5-61424b6f4129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2683</td>\n",
       "      <td>1462</td>\n",
       "      <td>3820</td>\n",
       "      <td>3172</td>\n",
       "      <td>2893</td>\n",
       "      <td>4139</td>\n",
       "      <td>43648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum</th>\n",
       "      <td>1711.17</td>\n",
       "      <td>870.84</td>\n",
       "      <td>1908.65</td>\n",
       "      <td>1063.56</td>\n",
       "      <td>812.92</td>\n",
       "      <td>764.32</td>\n",
       "      <td>1289.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0       1        2        3       4       5        6\n",
       "labels    B-ORG   I-LOC    I-ORG    B-LOC   B-PER   I-PER        O\n",
       "count      2683    1462     3820     3172    2893    4139    43648\n",
       "mean       0.64     0.6      0.5     0.34    0.28    0.18     0.03\n",
       "sum     1711.17  870.84  1908.65  1063.56  812.92  764.32  1289.42"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df_tokens.groupby(\"labels\")[[\"loss\"]] \n",
    "    .agg([\"count\", \"mean\", \"sum\"])\n",
    "    .droplevel(level=0, axis=1)\n",
    "    .sort_values(by=\"mean\", ascending=False)\n",
    "    .reset_index()\n",
    "    .round(2)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "481bcb00-5959-4509-a747-f3e6802bd16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a61855eb-30c9-4474-bf88-8f1e4570a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8c572e1a-60dd-4873-9786-edf187bbd98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "688037f2-f57c-4999-b949-c6d3b16333d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"],\n",
    "                      tags.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c28fbb25-62a5-4715-b3cd-264c8e4ed8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "def get_samples(df):\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        labels, preds, tokens, losses = [], [], [], []\n",
    "        for i, mask in enumerate(row[\"attention_mask\"]):\n",
    "            if i not in {0, len(row[\"attention_mask\"])}:\n",
    "                labels.append(row[\"labels\"][i])\n",
    "                preds.append(row[\"predicted_label\"][i])\n",
    "                tokens.append(row[\"input_tokens\"][i])\n",
    "                losses.append(f\"{row['loss'][i]:.2f}\")\n",
    "        df_tmp = pd.DataFrame({\"tokens\": tokens, \"labels\": labels, \n",
    "                               \"preds\": preds, \"losses\": losses}).T\n",
    "        yield df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8650cb9a-0c58-4458-8232-c884a45635ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁'</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁Τ</td>\n",
       "      <td>Κ</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁'</td>\n",
       "      <td>▁'</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁T</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁'</td>\n",
       "      <td>ri</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁'</td>\n",
       "      <td>k</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁'</td>\n",
       "      <td>ala</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.78</td>\n",
       "      <td>10.13</td>\n",
       "      <td>10.03</td>\n",
       "      <td>9.62</td>\n",
       "      <td>9.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.59</td>\n",
       "      <td>9.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.94</td>\n",
       "      <td>9.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1      2      3     4     5      6      7      8      9   \\\n",
       "tokens    ▁'   ▁''     ▁Τ      Κ   ▁''    ▁'     ▁'    ▁''     ▁T    ▁''   \n",
       "labels     O     O      O    IGN     O     O  B-LOC  I-LOC  I-LOC  I-LOC   \n",
       "preds      O     O  B-ORG  B-ORG     O     O      O      O      O      O   \n",
       "losses  0.00  0.00   3.91   0.00  0.00  0.00  10.78  10.13  10.03   9.62   \n",
       "\n",
       "           10    11     12     13    14     15     16    17    18  \n",
       "tokens     ▁'    ri    ▁''     ▁'     k    ▁''     ▁'   ala  </s>  \n",
       "labels  I-LOC   IGN  I-LOC  I-LOC   IGN  I-LOC  I-LOC   IGN   IGN  \n",
       "preds       O     O      O      O     O      O      O     O     O  \n",
       "losses   9.25  0.00   9.59   9.84  0.00   9.94   9.85  0.00  0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁''</td>\n",
       "      <td>8</td>\n",
       "      <td>.</td>\n",
       "      <td>▁Juli</td>\n",
       "      <td>▁''</td>\n",
       "      <td>▁:</td>\n",
       "      <td>▁Protest</td>\n",
       "      <td>camp</td>\n",
       "      <td>▁auf</td>\n",
       "      <td>▁dem</td>\n",
       "      <td>▁Gelände</td>\n",
       "      <td>▁der</td>\n",
       "      <td>▁Republika</td>\n",
       "      <td>n</td>\n",
       "      <td>ischen</td>\n",
       "      <td>▁Gar</td>\n",
       "      <td>de</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>7.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.89</td>\n",
       "      <td>8.04</td>\n",
       "      <td>8.60</td>\n",
       "      <td>6.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.27</td>\n",
       "      <td>8.74</td>\n",
       "      <td>7.71</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1     2      3      4      5         6     7      8      9   \\\n",
       "tokens    ▁''     8     .  ▁Juli    ▁''     ▁:  ▁Protest  camp   ▁auf   ▁dem   \n",
       "labels  B-ORG   IGN   IGN  I-ORG  I-ORG  I-ORG     I-ORG   IGN  I-ORG  I-ORG   \n",
       "preds       O     O     O      O      O      O         O     O      O      O   \n",
       "losses   7.72  0.00  0.00   5.89   8.04   8.60      6.70  0.00   8.27   8.74   \n",
       "\n",
       "              10     11          12     13      14     15     16    17  \n",
       "tokens  ▁Gelände   ▁der  ▁Republika      n  ischen   ▁Gar     de  </s>  \n",
       "labels     I-ORG  I-ORG       I-ORG    IGN     IGN  I-ORG    IGN   IGN  \n",
       "preds          O      O       B-ORG  I-ORG   I-ORG  I-ORG  I-ORG     O  \n",
       "losses      7.71   6.00        2.92   0.00    0.00   0.01   0.00  0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁United</td>\n",
       "      <td>▁Nations</td>\n",
       "      <td>▁Multi</td>\n",
       "      <td>dimensional</td>\n",
       "      <td>▁Integra</td>\n",
       "      <td>ted</td>\n",
       "      <td>▁Stabil</td>\n",
       "      <td>ization</td>\n",
       "      <td>▁Mission</td>\n",
       "      <td>▁in</td>\n",
       "      <td>▁the</td>\n",
       "      <td>▁Central</td>\n",
       "      <td>▁African</td>\n",
       "      <td>▁Republic</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>6.72</td>\n",
       "      <td>6.42</td>\n",
       "      <td>6.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.18</td>\n",
       "      <td>5.70</td>\n",
       "      <td>5.99</td>\n",
       "      <td>6.14</td>\n",
       "      <td>5.87</td>\n",
       "      <td>5.56</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1       2            3         4      5        6   \\\n",
       "tokens  ▁United  ▁Nations  ▁Multi  dimensional  ▁Integra    ted  ▁Stabil   \n",
       "labels    B-PER     I-PER   I-PER          IGN     I-PER    IGN    I-PER   \n",
       "preds     B-ORG     I-ORG   I-ORG        I-ORG     I-ORG  I-ORG    I-ORG   \n",
       "losses     6.72      6.42    6.53         0.00      6.38   0.00     6.25   \n",
       "\n",
       "             7         8      9      10        11        12         13     14  \n",
       "tokens  ization  ▁Mission    ▁in   ▁the  ▁Central  ▁African  ▁Republic   </s>  \n",
       "labels      IGN     I-PER  I-PER  I-PER     I-PER     I-PER      I-PER    IGN  \n",
       "preds     I-ORG     I-ORG  I-ORG  I-ORG     I-ORG     I-ORG      I-ORG  I-ORG  \n",
       "losses     0.00      6.18   5.70   5.99      6.14      5.87       5.56   0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"total_loss\"] = df[\"loss\"].apply(sum)\n",
    "df_tmp = df.sort_values(by=\"total_loss\", ascending=False).head(3)\n",
    "\n",
    "for sample in get_samples(df_tmp):\n",
    "    display(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f220b7c2-3d4e-43d0-86f2-5fb189dad161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁Ham</td>\n",
       "      <td>a</td>\n",
       "      <td>▁(</td>\n",
       "      <td>▁Unternehmen</td>\n",
       "      <td>▁)</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2             3      4      5\n",
       "tokens   ▁Ham      a     ▁(  ▁Unternehmen     ▁)   </s>\n",
       "labels  B-ORG    IGN  I-ORG         I-ORG  I-ORG    IGN\n",
       "preds   B-ORG  I-ORG  I-ORG         I-ORG  I-ORG  I-ORG\n",
       "losses   0.01   0.00   0.02          0.01   0.02   0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>▁Kesk</td>\n",
       "      <td>kül</td>\n",
       "      <td>a</td>\n",
       "      <td>▁(</td>\n",
       "      <td>▁Mart</td>\n",
       "      <td>na</td>\n",
       "      <td>▁)</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>losses</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2      3      4      5      6      7\n",
       "tokens  ▁Kesk    kül      a     ▁(  ▁Mart     na     ▁)   </s>\n",
       "labels  B-LOC    IGN    IGN  I-LOC  I-LOC    IGN  I-LOC    IGN\n",
       "preds   B-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC\n",
       "losses   0.02   0.00   0.00   0.02   0.02   0.00   0.02   0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide_output\n",
    "df_tmp = df.loc[df[\"input_tokens\"].apply(lambda x: u\"\\u2581(\" in x)].head(2)\n",
    "for sample in get_samples(df_tmp):\n",
    "    display(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd4fa23-8bc6-4b98-b957-b4b9debee6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
